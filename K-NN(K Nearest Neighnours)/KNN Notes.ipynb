{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of KNN Notes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXI1Y97SQcNl"
      },
      "source": [
        "**What will we learn?**\r\n",
        "1. **Introduction**: What is KNN\r\n",
        "2. **Correct value of K**: Finding the number of nearest neighbours\r\n",
        "3. **Distance Metric**: Understanding various Distance Metrices\r\n",
        "4. **Variations in KNN**: Uniform or weighted voting\r\n",
        "5. **Feature Scaling**\r\n",
        "6. **Scikit-learn Implementation**\r\n",
        "7. **Cross Validation**: Dividing the training data into k folds.\r\n",
        "8. **Self-implementation of KNN**: Code for KNN\r\n",
        "9. **Data preparation for KNN**\r\n",
        "10. **Curse of Dimensionality**: Removing irrelevant features\r\n",
        "11. **Other algorithms for KNN**: KDTree, BallTree\r\n",
        "12. **Advantages and Disadvantages of KNN**: Pros and cons of using KNN classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuQ6mvoeQcJ6"
      },
      "source": [
        "##**KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQKs47pSQcQZ"
      },
      "source": [
        "KNN stands for K-Nearest Neighbours. KNN is simple classification algorithm and it is generally used for datasets in which data points are separated into several classes and we have to predict the class for the new sample point.\r\n",
        "KNN is <b>non-parametric</b> and <b>lazy</b> learning algorithm.\r\n",
        "\r\n",
        "<b>Non-parametric</b> means that the algorithm does not make any assumptions on the given data distribution. Non-parametric covers technique that do not rely on data belonging to particular distribution and do not assume the structure of model to be fixed. So, KNN is used as classification algorithm in cases where we do not have much information about the distribution of the data.\r\n",
        "\r\n",
        "KNN is referred to as <b>Lazy</b> algorithm since is does not use training points to do any generalization, which means that there is no separate training phase. KNN keeps all the training data and uses most of the training data during the testing phase. Thus, KNN does not learn any model, it make predictions on the fly, computing similarity between testing point and each training data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMk87WnjQcTB"
      },
      "source": [
        "KNN algorithm is  based on <b>feature similarity</b>. We can classify the testing data point on the basis of resemblance of its features with that of the training data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7sVl62SQcVi"
      },
      "source": [
        "<img src=\"https://files.codingninjas.in/knn_intro-7470.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQyr6KTSJnP"
      },
      "source": [
        "The test sample (green circle) should either be classified into Class 1(blue squares) or Class 2 (red triangles). The class for the sample testing point is decided on the basis of majority vote-out.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krzBqlSRUduj"
      },
      "source": [
        "##**How to choose the correct value of 'K'?**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11HG3-sUU-o"
      },
      "source": [
        "'K' in KNN is a parameter that refers to the number of nearest neighbours to include in the majority of the voting process.\r\n",
        "\r\n",
        "If value of K is 1, the nearest training point belongs to Class 1, so we will say that the testing sample belongs to Class 1. Now, take the value of K to be 3, again using the methodology of majority vote, we will say that testing sample belongs to Class 2 (red traingles), since out of three nearest training data points, two belongs to Class 2 and one belong to Class 1.\r\n",
        "\r\n",
        "1. A very low value for K such as K = 1 or K = 2, can be noisy and lead to the effects of outliers in the model.\r\n",
        "2. Inversely, as we increase the value of K, our predictions become more stable due to majority voting / averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\r\n",
        "3. In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make K an odd number to have a tiebreaker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLIrBfNZSJg-"
      },
      "source": [
        "KNN can be used for Classification as well as Regression.\r\n",
        "\r\n",
        "While using KNN for classification - output is a class membership, we will classify the sample point using the technique of majority vote among its neighbors and the most common class among its K nearest neighbours is assigned to the testing point. In regression, output is the property value of object, this value is average or median of the value of its K nearest neighbors.\r\n",
        "\r\n",
        "In classification, KNN is used to predict a class which is a discrete value whereas in regression, KNN predicts continuous values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr23pYV7SJcR"
      },
      "source": [
        "###**Why don't we choose value value of K to be 1 ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F14fhzC3SJYG"
      },
      "source": [
        "Choosing the value of K to be 1 makes our model more prone to outliers and overfitting. Value of 1 means that we will consider only the closest (or nearest) neighbor to predict the class for our testing sample, and in majority of the cases it will lead to overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usnTxG-6SJSz"
      },
      "source": [
        "<img src=\"https://files.codingninjas.in/knn-1-7479.jpg\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTSdRYv5SJKE"
      },
      "source": [
        "Consider the case here, if we choose value of K to be 1, the given training sample will get classified as dot (Class 2), instead of being classified as a cross (Class 1). So, in such cases certain optimal value of K should be chosen to get good results. Choosing the value of K to be 1, leads to formation of complex decision boundaries and hence will lead to overfitting.\r\n",
        "\r\n",
        "Since we will be using the majority vote technique, so value of K is taken to be odd, to obtain clear result about the class of the testing data sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKkyugO7Z0RW"
      },
      "source": [
        "##**Distance Metric for KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fVQY25pZ0NS"
      },
      "source": [
        "There are various distance metrices that can be chosen such as Manhattan Distance, Euclidian Distance, etc.\r\n",
        "\r\n",
        "$$Manhattan \\;Distance = |\\sum_{i=1}^n X_1^i - X_2^i|$$\r\n",
        "$$Euclidian\\; Distance = \\sqrt{(\\sum_{i=1}^n (X_1^i-X_2^i)^2)} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvQ5cWfiZ0JT"
      },
      "source": [
        "Where $X_1$ and $X_2$ are two different data points and $i$ traverses over all the features in the given dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2MTqTyEZ0Fk"
      },
      "source": [
        "##**Variations in KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQwNeYfsZz-t"
      },
      "source": [
        "Variations in KNN are possible on the basis of how neighbouring points are going to vote. The weight of the vote of the testing point(s) is inversely proportional to its distance from the testing point. We can go either with uniform voting or with weighted voting. In case of weighted voting, the point nearer to the testing sample will have a larger say in the vote as compared to the point which is farther away from the testing point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3riFHG7ZZzz0"
      },
      "source": [
        "##**Feature Scaling before KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QI5AhGCdv3M"
      },
      "source": [
        "In KNN, we are looking for points which are closest to the testing point. In case we do not perform feature scaling, if we have value of one feature in thousands and value of other feature in smaller units, then the effect of first feature will completely overpower and dominate over the effect of second feature in the final output. So, it is of utmost importance to apply feature scaling before applying KNN so that all the features have equal contribution in the final predicted output for the given testing point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmZnqefvdvz8"
      },
      "source": [
        "##**KNN in Sklearn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlPpe__Ddvvy"
      },
      "source": [
        "We will be applying KNN on Breast Cancer Dataset and use inbuilt KNN Classifier inside sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq8pLV-tZtyI"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP-qJWagd-Un"
      },
      "source": [
        "cancer = datasets.load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QIgDOpqeAbg",
        "outputId": "32851e8b-1285-48c8-a39b-53c55f4edcf6"
      },
      "source": [
        "cancer.feature_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p555DU_peCkk",
        "outputId": "574cf949-587e-4b81-ef4e-f06566bb5ecc"
      },
      "source": [
        "cancer.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaohvZl9eEi4"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPVgIETHeObJ",
        "outputId": "207403d2-4c7f-47f5-af39-0ec3376ddb55"
      },
      "source": [
        "clf = KNeighborsClassifier()\r\n",
        "clf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrOp4RHLeQDR",
        "outputId": "a753e26b-fc58-4ab0-a4b1-90a7f7a7d203"
      },
      "source": [
        "clf.score(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9385964912280702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVyJNPfeW-p"
      },
      "source": [
        "Default value of K (number of neighbors) is equal to 5 in Sklearn. By default, Sklearn implements Minkowski distance metric. General form of Minkowski distance is :\r\n",
        "$$Minkowski \\;Distance = |\\sum_{i=1}^n (X_1^i - X_2^i)^p|^{\\frac{1}{p}}$$\r\n",
        "If p = 1, that means we are using Manhattan Distance and if p = 2, that means we are using Euclidian Distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naYDmskViw6v"
      },
      "source": [
        "##**Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9vNjIsi0oF"
      },
      "source": [
        "The best value of K is the one for which we get lowest error on testing data. So, what we can do is to repeatedly train our model using both training and testing data, for different values of parameter K and then finally choose value of K which results in minimum error. But in this process, we are using the testing data as a part of our training process to obtain the optimal value of K. Hence, this process is not to be used. On the other hand if we use only training data and tune the value of parameter K, it will lead to overfitting. This will result in lower value of error on training data, but comparatively higher value of error on the testing data.\r\n",
        "\r\n",
        "So, to obtain the optimal value of K, we use a method known as CROSS VALIDATION. Cross Validation basically means taking out the subset from the training data and not using this subset in the training process. This subset of training data is called the 'validation set'. There are various techniques available for cross validation, we will be using the most general one, known as K-fold cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L84s62zci0mm"
      },
      "source": [
        "<img src=\"https://files.codingninjas.in/knn_crossval-7474.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCbEfOaGi0je"
      },
      "source": [
        "In K-fold cross validation, the training data is randomly split into K different samples (or folds). One of the sample is taken to be the validation set and the model is fitted on the remaining (K - 1) samples. The accuracy of the model is then computed. The same process is repeated K times, each time taking a different sample of points to be in the validation set. This results in K values for test error and these values are averaged out to obtain the overall result.\r\n",
        "\r\n",
        "Cross Validation is used to estimate the test error and generate more robust models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if94xFGwi0hs"
      },
      "source": [
        "###**Cross Validation in Sklearn**\r\n",
        "\r\n",
        "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCnQ5NOaizsS",
        "outputId": "43e62262-6e6a-4dab-e322-f5d4c29025ed"
      },
      "source": [
        "x_axis = []\r\n",
        "k_scores = []\r\n",
        "for k in range(1,50, 2):\r\n",
        "    x_axis.append(k)\r\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\r\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=10, scoring='accuracy')\r\n",
        "    k_scores.append(scores.mean())\r\n",
        "    \r\n",
        "    #Printing values\r\n",
        "    print(\"K = \",k)\r\n",
        "    print(\"Scores : \")\r\n",
        "    print(scores)\r\n",
        "    print(\"Mean Score = \",scores.mean())\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K =  1\n",
            "Scores : \n",
            "[0.93478261 0.95652174 0.91304348 0.84782609 0.86956522 0.93333333\n",
            " 0.97777778 0.91111111 0.93333333 0.93333333]\n",
            "Mean Score =  0.9210628019323671\n",
            "\n",
            "K =  3\n",
            "Scores : \n",
            "[0.91304348 0.95652174 0.86956522 0.93478261 0.89130435 0.95555556\n",
            " 0.95555556 0.88888889 0.95555556 0.95555556]\n",
            "Mean Score =  0.927632850241546\n",
            "\n",
            "K =  5\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.86956522 0.91304348 0.91304348 0.93333333\n",
            " 0.97777778 0.86666667 0.95555556 0.91111111]\n",
            "Mean Score =  0.9274879227053141\n",
            "\n",
            "K =  7\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.91304348 0.89130435 0.97777778\n",
            " 0.97777778 0.86666667 0.95555556 0.93333333]\n",
            "Mean Score =  0.9319806763285025\n",
            "\n",
            "K =  9\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.91304348 0.86956522 0.97777778\n",
            " 0.97777778 0.86666667 0.95555556 0.95555556]\n",
            "Mean Score =  0.9320289855072463\n",
            "\n",
            "K =  11\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.91304348 0.91304348 0.89130435 0.97777778\n",
            " 0.97777778 0.86666667 0.93333333 0.93333333]\n",
            "Mean Score =  0.9319323671497586\n",
            "\n",
            "K =  13\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.91304348 0.89130435 0.97777778\n",
            " 0.97777778 0.86666667 0.95555556 0.93333333]\n",
            "Mean Score =  0.9319806763285025\n",
            "\n",
            "K =  15\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.86956522 0.91304348 0.89130435 0.97777778\n",
            " 0.97777778 0.88888889 0.93333333 0.91111111]\n",
            "Mean Score =  0.9254106280193237\n",
            "\n",
            "K =  17\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.86956522 0.93478261 0.89130435 0.97777778\n",
            " 0.97777778 0.88888889 0.93333333 0.91111111]\n",
            "Mean Score =  0.9275845410628019\n",
            "\n",
            "K =  19\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.86956522 0.91304348 0.86956522 0.97777778\n",
            " 0.97777778 0.88888889 0.93333333 0.91111111]\n",
            "Mean Score =  0.9232367149758455\n",
            "\n",
            "K =  21\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.86956522 0.91304348 0.86956522 0.97777778\n",
            " 0.97777778 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9210144927536232\n",
            "\n",
            "K =  23\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.86956522 0.91304348 0.86956522 0.97777778\n",
            " 0.97777778 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.918792270531401\n",
            "\n",
            "K =  25\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.86956522 0.91304348 0.84782609 0.97777778\n",
            " 0.97777778 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.9166183574879228\n",
            "\n",
            "K =  27\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.89130435 0.91304348 0.84782609 0.95555556\n",
            " 0.97777778 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.9165700483091788\n",
            "\n",
            "K =  29\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.89130435 0.91304348 0.84782609 0.95555556\n",
            " 0.97777778 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.9165700483091788\n",
            "\n",
            "K =  31\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.89130435 0.91304348 0.84782609 0.95555556\n",
            " 0.97777778 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.9143961352657005\n",
            "\n",
            "K =  33\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.89130435 0.91304348 0.84782609 0.95555556\n",
            " 0.97777778 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.9143961352657005\n",
            "\n",
            "K =  35\n",
            "Scores : \n",
            "[0.95652174 0.93478261 0.89130435 0.91304348 0.84782609 0.95555556\n",
            " 0.97777778 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.918792270531401\n",
            "\n",
            "K =  37\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.91304348 0.91304348 0.84782609 0.95555556\n",
            " 0.95555556 0.84444444 0.93333333 0.91111111]\n",
            "Mean Score =  0.9143478260869566\n",
            "\n",
            "K =  39\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.89130435 0.89130435 0.84782609 0.95555556\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9122222222222224\n",
            "\n",
            "K =  41\n",
            "Scores : \n",
            "[0.93478261 0.91304348 0.91304348 0.86956522 0.86956522 0.95555556\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9122222222222222\n",
            "\n",
            "K =  43\n",
            "Scores : \n",
            "[0.93478261 0.91304348 0.91304348 0.86956522 0.84782609 0.95555556\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.910048309178744\n",
            "\n",
            "K =  45\n",
            "Scores : \n",
            "[0.91304348 0.91304348 0.91304348 0.86956522 0.84782609 0.95555556\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9078743961352658\n",
            "\n",
            "K =  47\n",
            "Scores : \n",
            "[0.91304348 0.91304348 0.93478261 0.89130435 0.84782609 0.95555556\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9122222222222224\n",
            "\n",
            "K =  49\n",
            "Scores : \n",
            "[0.91304348 0.93478261 0.93478261 0.89130435 0.84782609 0.95555556\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9143961352657005\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtmiuHL7nRyD"
      },
      "source": [
        "When the cv argument is an integer, cross_val_score uses the K-Fold or Stratified K-Fold strategies by default. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "eQM95fBkmieq",
        "outputId": "2e1c1182-7d44-493b-9987-a46f4693516e"
      },
      "source": [
        "plt.plot(x_axis, k_scores)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2TUvomnn3XU",
        "outputId": "3b8ee23e-0dfc-4634-aca4-bade7edd7812"
      },
      "source": [
        "optimal_k = x_axis[k_scores.index(max(k_scores))]\r\n",
        "optimal_k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGGf76sxaoB"
      },
      "source": [
        "**K-Fold and  Stratified K-Fold**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ndjt_0moTHt"
      },
      "source": [
        "K-Fold divides all the samples in $k$ groups of samples, called folds. The prediction function is learned using $k-1$ folds, and the fold left out is used for test. \r\n",
        "For More Details , refer to the documentation : \r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html?highlight=kfold#sklearn.model_selection.KFold\r\n",
        "\r\n",
        "Stratified K-Fold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\r\n",
        "\r\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HNPGgHaoT1Q",
        "outputId": "bd33b885-119d-4f16-bb4f-5a5ff8a5b389"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "x_axis = []\r\n",
        "k_scores = []\r\n",
        "for k in range(1,50, 2):\r\n",
        "    x_axis.append(k)\r\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\r\n",
        "    scores = cross_val_score(clf, x_train, y_train, cv=KFold(10), scoring='accuracy')\r\n",
        "    k_scores.append(scores.mean())\r\n",
        "    \r\n",
        "    #Printing values\r\n",
        "    print(\"K = \",k)\r\n",
        "    print(\"Scores : \")\r\n",
        "    print(scores)\r\n",
        "    print(\"Mean Score = \",scores.mean())\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K =  1\n",
            "Scores : \n",
            "[0.91304348 0.97826087 0.93478261 0.84782609 0.82608696 0.86666667\n",
            " 0.95555556 0.93333333 0.93333333 0.93333333]\n",
            "Mean Score =  0.9122222222222222\n",
            "\n",
            "K =  3\n",
            "Scores : \n",
            "[0.91304348 0.97826087 0.89130435 0.93478261 0.84782609 0.91111111\n",
            " 0.93333333 0.93333333 0.95555556 0.95555556]\n",
            "Mean Score =  0.9254106280193237\n",
            "\n",
            "K =  5\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.89130435 0.91304348 0.89130435 0.91111111\n",
            " 0.95555556 0.88888889 0.95555556 0.91111111]\n",
            "Mean Score =  0.9252657004830919\n",
            "\n",
            "K =  7\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.89130435 0.91304348 0.86956522 0.95555556\n",
            " 0.95555556 0.88888889 0.95555556 0.93333333]\n",
            "Mean Score =  0.9297584541062802\n",
            "\n",
            "K =  9\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.89130435 0.91304348 0.86956522 0.95555556\n",
            " 0.95555556 0.88888889 0.95555556 0.95555556]\n",
            "Mean Score =  0.9319806763285025\n",
            "\n",
            "K =  11\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.91304348 0.91304348 0.84782609 0.95555556\n",
            " 0.95555556 0.88888889 0.93333333 0.93333333]\n",
            "Mean Score =  0.927536231884058\n",
            "\n",
            "K =  13\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.89130435 0.93478261 0.86956522 0.95555556\n",
            " 0.95555556 0.88888889 0.95555556 0.93333333]\n",
            "Mean Score =  0.9319323671497586\n",
            "\n",
            "K =  15\n",
            "Scores : \n",
            "[0.95652174 0.97826087 0.86956522 0.93478261 0.89130435 0.95555556\n",
            " 0.95555556 0.91111111 0.93333333 0.91111111]\n",
            "Mean Score =  0.9297101449275363\n",
            "\n",
            "K =  17\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.86956522 0.93478261 0.89130435 0.95555556\n",
            " 0.95555556 0.91111111 0.93333333 0.91111111]\n",
            "Mean Score =  0.927536231884058\n",
            "\n",
            "K =  19\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.86956522 0.93478261 0.86956522 0.93333333\n",
            " 0.95555556 0.91111111 0.93333333 0.91111111]\n",
            "Mean Score =  0.9231400966183575\n",
            "\n",
            "K =  21\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.86956522 0.93478261 0.86956522 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  23\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.86956522 0.93478261 0.86956522 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  25\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.86956522 0.93478261 0.86956522 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  27\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.93478261 0.84782609 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  29\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.93478261 0.84782609 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  31\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.93478261 0.84782609 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  33\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.93478261 0.84782609 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  35\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.93478261 0.84782609 0.91111111\n",
            " 0.95555556 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9164734299516908\n",
            "\n",
            "K =  37\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.89130435 0.91304348 0.84782609 0.91111111\n",
            " 0.93333333 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9120772946859903\n",
            "\n",
            "K =  39\n",
            "Scores : \n",
            "[0.95652174 0.95652174 0.91304348 0.91304348 0.84782609 0.91111111\n",
            " 0.93333333 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9142512077294687\n",
            "\n",
            "K =  41\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.91304348 0.91304348 0.84782609 0.91111111\n",
            " 0.93333333 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9099033816425122\n",
            "\n",
            "K =  43\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.93478261 0.89130435 0.84782609 0.91111111\n",
            " 0.93333333 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9099033816425122\n",
            "\n",
            "K =  45\n",
            "Scores : \n",
            "[0.93478261 0.93478261 0.91304348 0.89130435 0.84782609 0.91111111\n",
            " 0.93333333 0.88888889 0.93333333 0.91111111]\n",
            "Mean Score =  0.909951690821256\n",
            "\n",
            "K =  47\n",
            "Scores : \n",
            "[0.93478261 0.95652174 0.91304348 0.89130435 0.84782609 0.91111111\n",
            " 0.93333333 0.88888889 0.93333333 0.91111111]\n",
            "Mean Score =  0.9121256038647344\n",
            "\n",
            "K =  49\n",
            "Scores : \n",
            "[0.93478261 0.95652174 0.93478261 0.89130435 0.84782609 0.91111111\n",
            " 0.93333333 0.86666667 0.93333333 0.91111111]\n",
            "Mean Score =  0.9120772946859903\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "awCt4JUeozdm",
        "outputId": "bbffba30-ce30-4573-a82f-7399c3a0d704"
      },
      "source": [
        "plt.plot(x_axis, k_scores)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL0I5m0Go2Fy",
        "outputId": "eeb5c346-e515-4fa5-d56f-c48105995a63"
      },
      "source": [
        "optimal_k = x_axis[k_scores.index(max(k_scores))]\r\n",
        "optimal_k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJzxUthEpDS_"
      },
      "source": [
        "Using this way, we can choose the optimal value of K using cross validation. If the value of K is very less, for eg, say 1, it will lead to overfitting and will result in formation of complex decision boundaries. On the the hand, if the value of K is very high, we are basically underfitting and not actually taking class of neighbours into consideration. In this case, we predict the class of the testing sample from the majority class of the overall training data rather than from the majority class given by the neighbors.\r\n",
        "\r\n",
        "Lower values of K corresponds to low bias but high variance and result in jagged and complex decision boundaries, while higher value of K corresponds to lower variance but increased bias and leads to formation of smoother decision boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJoSC0DYpDRK"
      },
      "source": [
        "![picture](https://files.codingninjas.in/knn_dec1-7476.png)  ![picture](https://files.codingninjas.in/knn_dec20-7477.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vklZtroxpDMq"
      },
      "source": [
        "##**Self Implementation of KNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjF6_d7Sp8NG"
      },
      "source": [
        "from sklearn import datasets\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from collections import Counter\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bibCRul3p8LJ"
      },
      "source": [
        "cancer = datasets.load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRE7lg5Dp8IK"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbZ5JaDGp8Gs",
        "outputId": "52e6c269-f142-45ea-d844-15f2dcdb58d7"
      },
      "source": [
        "clf = KNeighborsClassifier(n_neighbors = 7)\r\n",
        "clf.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PINZ6b1CqDt1",
        "outputId": "ca244839-c70f-4163-ebb4-60cc87779224"
      },
      "source": [
        "clf.score(X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu0gHp4JqD0_"
      },
      "source": [
        "def train(x,y):\r\n",
        "    return\r\n",
        "\r\n",
        "def predict_one(x_train, y_train, x_test, k):\r\n",
        "    distances = []\r\n",
        "    for i in range(len(x_train)):\r\n",
        "        distance = ((x_train[i,:] - x_test)**2).sum()\r\n",
        "        distances.append([distance,i])\r\n",
        "    distances = sorted(distances)\r\n",
        "    targets = []\r\n",
        "    for i in range(k):\r\n",
        "        index_of_training_data = distances[i][1]\r\n",
        "        targets.append(y_train[index_of_training_data])\r\n",
        "    return Counter(targets).most_common(1)[0][0]\r\n",
        "    \r\n",
        "def predict(x_train, y_train, x_test_data, k):\r\n",
        "    predictions = []\r\n",
        "    for x_test in x_test_data:\r\n",
        "        predictions.append(predict_one(x_train, y_train, x_test, k))\r\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b5oovW4qD2x",
        "outputId": "86469df9-ffc1-437a-c9b2-784ad0657ec0"
      },
      "source": [
        "y_pred = predict(X_train, Y_train, X_test, 7)\r\n",
        "accuracy_score(Y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZfaCDGZNAWl"
      },
      "source": [
        "##**Prepare Data For KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-vkAPbGNASw"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   **Rescale Data**: KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution \r\n",
        "*   **Address Missing Data**: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed\r\n",
        "* **Lower Dimensionality**: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0dH8X2BpDId"
      },
      "source": [
        "##**Curse of Dimensionality**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QS-kiqIpDGx"
      },
      "source": [
        "In case of KNN, if our data has more irrelevant features, then it will lead to bad results. Also, if we have two correlated features, then both of these take part in the calculations and net effect due to addition of these becomes double, and we might up getting wrong results.\r\n",
        "\r\n",
        "We can do following things to solve this problem :\r\n",
        "1. **Assign weights to features**\r\n",
        "$$\\sum_{i=1}^n W_i * (X_1^i- X_2^i)^2$$\r\n",
        "This can be used as the distance metric, to calculate distance between $X_1$ and $X_2$, where $W_i$ is the weight assigned to the $i^{th}$ feature. These weights can be choosen randomly or can be calculated using Gradient Descent and deciding upon the cost function which is to be minimised and hence, obtain these weights.\r\n",
        "\r\n",
        "   This proves to be benificial in the cases where two features are highly correlated, in that case either both these features are going to get high weights or one is going to get high weight and other is going to get the lower weight. And, therefore, we can obtain better results using this technique.\r\n",
        "\r\n",
        "2. **Feature Selection**\r\n",
        "\r\n",
        "   We will apply feature selection before using KNN Classifier. Backward Elimination is the technique used for doing feature selection, using which we will keep few features and get rid of other features from all the features intially in the dataset. In this technique, we will traverse over all the features and obtain the accuracy score by keeping this feature and next time, by removing this feature. If the accuracy of the model improves on removing this feature, then we will exclude this feature, otherwise we will keep this feature and likewise do the same thing for all the features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gwvUruepDDh"
      },
      "source": [
        "##**Other Algorithms for KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW8mitEcpDBg"
      },
      "source": [
        "KDTree and BallTree can be used to improve the performance the model. The basic intuition behind these is to build something like Binary Search Tree using the training data and then for the testing data traverse through the whole tree and on reaching the leaf node, obtain the K Nearest Neighbors from all the nodes traversed by the testing sample point.\r\n",
        "\r\n",
        "For more details about these algorithms, refer to the documentation link http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\r\n",
        "\r\n",
        "KDTree, BallTree, brute force method and other algorithms can be used depending upon the data. Applying various of these methods, we can see the results and compare their performance with different algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ApInR1GpC-m"
      },
      "source": [
        "##**Advantages and Disadvatages of KNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezObKHP7pC8w"
      },
      "source": [
        "###**Advantages**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lunr8lbGs03c"
      },
      "source": [
        "1. Easy to understand and code.\r\n",
        "2. Works well for multi-class classification.\r\n",
        "3. Insensitive to outliers for optimal choice for value of K, but accuracy can be affected from noise and irrelevant features.\r\n",
        "4. Versatile i.e. can work for both classification and regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMnFNXe0s742"
      },
      "source": [
        "###**Disadvantages**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6JPZg-5s-6J"
      },
      "source": [
        "1. Computationally expensive, because the algorithm stores all the training data\r\n",
        "2. Testing time is huge.\r\n",
        "3. Sensitive to irrelevant features and scaling.\r\n",
        "4. If training data split is biased i.e. majority of data points belong to particular class, then KNN also gets biased."
      ]
    }
  ]
}