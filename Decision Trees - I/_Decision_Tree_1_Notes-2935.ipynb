{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "_Decision Tree - 1 Notes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qIOHgFT7JFK"
      },
      "source": [
        "# **Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFX907Kq7JH7"
      },
      "source": [
        "**What will you learn?**\n",
        "1. **Definiton and working**: An introduction to Decision Tree\n",
        "2. **Working Example**: Building tree for Interview Call\n",
        "3. **Algoriths for Decision Trees**: What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?\n",
        "3. **Getting to Best Decision Tree**: Know where to begin and when to stop.\n",
        "4. **Deciding Feature to Split On**: Which feature helps us improve metrics.\n",
        "5. **Continous Values Features**: How to split data in case of Continous Values\n",
        "6. **Advantages and Disadvantages**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBco1Lrx7JJ5"
      },
      "source": [
        "A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression. A decision tree is a tree-like graph which uses a branching method to illustrate every possible outcome of a decision. \n",
        "\n",
        "Its structure is such that the nodes represent the place where we pick an attribute and ask a question. The edges represent the answers to the question and the leaves represent the actual output or class label. They are used in non-linear decision making with simple linear decision surface.\n",
        "\n",
        "Decision trees classify the examples by sorting them down the tree from the root to some leaf node, with the leaf node providing the classification to the example. Each node in the tree acts as a test case for some attribute, and each edge descending from that node corresponds to one of the possible answers to the test case. This process is recursive in nature and is repeated for every subtree rooted at the new nodes.\n",
        "\n",
        "Lets start by taking a example of OR of two variables X1 and X2. The decison tree for the same is shown below :\n",
        "\n",
        "<img src = \"https://files.codingninjas.in/or_dt-7208.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Zri2y77JLt"
      },
      "source": [
        "We have total four possible combinations as shown in the truth table. So at the beginning we have 2 True and 2 False outcomes to start with. The first condition we check for is the value of X1. If X1 is true then we are sure that the result will be true and hence we arrive at 2 true and no false. Note that ,we only consider those rows of the table which are having X1 as true i.e. the first and the third row in the current example .That is why we have 2 total outcomes out of which 2 are true and none is false. This node is having a definite answer.If we arrive at this node there is no confusion as to which what our tree should return, therefore these are called **pure nodes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vamj3hUw7JOG"
      },
      "source": [
        "If X1 is false then the answer is dependant on the value of X2. If X2 is false the answer is false and if the answer is true the final result is true. We have three pure nodes here as shown in the pictorial representation of the decision tree. Now if we are given any new data for prediction we just need to run it through our tree. For example if we get A and B as two values for testing. We check first if A is true. If A is true we have true as our answer else we need to check for the value of B. If B is true then our answer agin is true else we have false as the final outcome. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXESOZs4-fOg"
      },
      "source": [
        "## **Another Example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_WDzA-n-fml"
      },
      "source": [
        "Lets consider another example which is not as straight forward as the previous one. Suppose we need to predict that if a person will get interview call or not based on some factors. There can be many factors but for simplicity lets consider that we focus on the level of projects, good inten and whether the person is from top 50 colleges or not.\n",
        "Unlike the previous example in which we just picked X1 for the first decision, it is quite arguable that which factor should we pick first here. Lets assume that we start by picking whether the person has done a good internship or not(which is in the form of true or false).\n",
        "\n",
        "<img src = \"\thttps://files.codingninjas.in/dt_interview-7209.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw7J-V6v7JQe"
      },
      "source": [
        "Similarly we continue to break down the nodes further on the basis of the features which are still left(type of college and projects in this case ).\n",
        "Consider a case where we have used up all the features and still we do not arrive at a pure node. Cases like this can surely be true. For eg: we may have three students lacking in all the three fields but still one gets the interview call and the other two do not. In such a case, our node which corresponds to \"no\" at each of the above three decisions cannot give a pure no as answer i.e. it is not a pure node. What should we do then? As we do not have anymore features left to judge on, we can simply favour the majority class of the node.\n",
        "Therefore, there are two cases when we need to stop with the breaking of nodes into subparts, those are :\n",
        "1. When we have a pure node, there is no need to proceed further.\n",
        "2. When all the features are used and we dont have any other feature present.\n",
        "Depending upon which factor we pick to split on we can have different Decision trees and their accuracy will also vary. We will discuss how to pick a decent tree in the next section. What we should note is that till now we have only considered the examples where we have binary( true or false ) as outcomes. If a partucular feature has n different values, we may have to break a node into n subnodes. If a feature has all unique entry then we reach the pure nodes in one step but for many cases that is useless. For example, dividing a class of students on the basis of their roll number into subnodes may result in all pure nodes but it is of no use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ_K3IwrHd-V"
      },
      "source": [
        "##**Various Algorithms for Decision Trees**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwfm_n7nHeQe"
      },
      "source": [
        "**ID3 (Iterative Dichotomiser 3)** was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
        "\n",
        "**C4.5** is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
        "\n",
        "**C5.0** is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
        "\n",
        "**CART (Classification and Regression Trees)** is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
        "\n",
        "**scikit-learn uses an optimised version of the CART algorithm;** however, scikit-learn implementation does not support categorical variables for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhvzr23E7JVS"
      },
      "source": [
        "## **Getting to Best Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf-I-V0B7Jcb"
      },
      "source": [
        "Building a decision tree involves deciding on which feature to choose and what condition to use on splitting, alongwith knowing when to stop. In the first split on the root, all features are considered and the data points are divided into groups based on this split. Lets suppose, we have n features. Then we will be having n candidate splits at the first level. Now, we will calculate how much accuracy each split will cost us, using a function. The split(feature) which results in maximum accuracy is choosen at this level and data points are divided into child nodes according to that feature only. The child nodes formed are recursively divided into deeper levels, resulting in formation of the entire tree.\n",
        "\n",
        "In the case we have n features, then we can possibly make exponential number of decision trees. It is categorised into NP-HARD Problem. For finding out the best tree all possible combinations of tree possible should be taken care off. So, we are interested here to find out the good tree and not the best one. Using the **GREEDY approach**, we will try to lower the cost (and also maximize the accuracy) and according to this, build a good decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mOXTSqH7JfB"
      },
      "source": [
        "## **Deciding Features to Split On (On the Basis of Accuracy)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcE_PjA-7JhT"
      },
      "source": [
        "Taking the same example of the student applying for an internship in college will get a call for the interview or not. Considering total 50 students, we will pick up all the features one by one and see how many mistakes we make at each level, taking the decision on the basis of majority. Majority decision means that if we have 10 YES and 40 NO in particular node, then we will take our decision as NO for that node.\n",
        "\n",
        "<img src = \"https://files.codingninjas.in/greatintern-7331.jpg\" width = \"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9274g1GM7Jj3"
      },
      "source": [
        "At the root node, we will take our decision as NO according to majority, then we make a total of 10 mistakes. So, it is represented as 10/50(10 out of 50) are wrong decisions.\n",
        "Furthur, if we split the data points on the feature \"Great Intern\", then left node represents students who are great interns, total of 8 students, out of which 5 got a call for interview and 3 didn't. Right node similarly reprents students who are not great interns, total of 42 students, and again out of which 5 got a call for interview and 37 didn't. So, we take our decision as YES for left side child node and NO for side side child node (according to majority only). And we make a total of 8(3+5) mistakes at this level, which actually means that before using this features, if we took decision at root node only we make 10 mistakes in our decision, while after splitting on this feature we make a total of 8 mistakes at this level. Since, we are making less number of mistakes in our decision at lower level, so it will be benificial and serve as an advantage in decision making if we make a split at this level.\n",
        "\n",
        "<img src = \"https://files.codingninjas.in/topcollege-7333.jpg\" width = \"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_pYPW-7JmR"
      },
      "source": [
        "Similarly, here we are making 10 mistakes at the root level, but after splitting we are making a total of 6(2+4) mistakes in decision at the lower level, and hence this split is also favoured at this step of splitting.\n",
        "\n",
        "<img src = \"\thttps://files.codingninjas.in/projects-7332.jpg\" width = \"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAC12xcl7Jq7"
      },
      "source": [
        "Here, we are making total of 10 mistakes at root level, but after splitting also we are making 10(5+5) mistakes, so we might possibly avoid split using this feature.\n",
        "\n",
        "We will make a split using that feature by which number of mistakes made at the lower level of the tree gets reduced after a split is performed using that feature. Therefore, here we will make a split using second feature i.e. Top College"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXczNpzx7Jtg"
      },
      "source": [
        "## **Handle Discrete and Continuous Value Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrvcL6cE7Jv1"
      },
      "source": [
        "If we have discrete value feature, say labelled data for example gender of a person we have Male and Females. Now, making a split on gender of person results in 2 child nodes, one for males and other for females.\n",
        "Consider continuous valued features, say salary. Every person has different salary and values are spread over a wide range. If we have to make a split on Salary, then an option is to make different child node for every different value of salary we obtain. But unfortunately, it will result in large overfitting of data.\n",
        "So, to avoid this difficulty for continuous value features. We follow the procedure mentioned to achieve the better split using this feature.\n",
        "\n",
        "1. Spread all the salaries(values for feature choosen) on the straight line from lowest to highest order.\n",
        "2. Split the data according to mid point values, taking all the pairwise adjacent points.\n",
        "3. Take the salary value for that particular split that results in maximum accuracy or minimum mistakes made while making the decision.\n",
        "Example, we have 4 value of salaries, 5000, 10000, 20000 and 50000. We take the mid point of all these which comes out to be 7500, 15000 and 35000. We will make a split on all these salary values one by one, first on 7500 salary, which means people with salary less than 7500 come on left side and all others on the right side. Similarly, doing for salary values 15000 and 35000. We choose that particular salary value to make a split at this level which results in maximum accuracy.\n",
        "\n",
        "This process is followed for making a binary split for the continuous value features. We can choose salary feature again at the deeper level and make a split again using this feature below, with decreased range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nnlCBHTAk04"
      },
      "source": [
        "## **Advantages of Decision Trees**\n",
        "1. Decision trees are simple to understand, interpret, visualize.\n",
        "2. Decision trees can handle both numerical and categorical data. They can also handle multi-output problems.\n",
        "3. Decision trees require relatively little effort from users for data preparation.\n",
        "4. Nonlinear relationships between parameters do not affect tree performance.\n",
        "5. Decision trees provide a clear indication of which fields are most important for prediction or classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54j1ropiAkcV"
      },
      "source": [
        "## **Disdvantages of Decision Trees**\n",
        "\n",
        "1. Decision tree learners can create over-complex trees that do not generalize the data well. This is called overfitting.\n",
        "2. Decision trees are less appropriate for estimation tasks where the goal is to predict the value of a continuous attribute.\n",
        "3. Decision trees are prone to errors in classification problems with many class and relatively small number of training examples.\n",
        "4. Decision tree can be computationally expensive to train. The process of growing a decision tree is computationally expensive. At each node, each candidate splitting field must be sorted before its best split can be found. In some algorithms, combinations of fields are used and a search must be made for optimal combining weights. Pruning algorithms can also be expensive since many candidate sub-trees must be formed and compared.\n",
        "5. Greedy algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees, where the features and samples are randomly sampled with replacement.\n",
        "6. Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the data set prior to fitting with the decision tree."
      ]
    }
  ]
}